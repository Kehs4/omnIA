import express from "express";
import fs from "fs";
import cors from "cors";
import 'dotenv/config'; // Carrega as variÃ¡veis de ambiente do arquivo .env
import { GoogleGenerativeAI } from "@google/generative-ai";

const app = express();
const PORT = 3000;

// --- CONFIGURAÃ‡ÃƒO DA IA GENERATIVA (GEMINI) ---
// Adiciona uma verificaÃ§Ã£o para garantir que a chave da API foi carregada
if (!process.env.GEMINI_API_KEY) {
    console.error("ERRO FATAL: A variÃ¡vel de ambiente GEMINI_API_KEY nÃ£o foi encontrada.");
    process.exit(1); // Encerra o processo se a chave nÃ£o existir
}
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
const model = genAI.getGenerativeModel({ model: "gemini-2.5-flash" });
// -----------------------------------------

app.use(cors()); // Permite que o frontend (em outra porta) acesse a API
app.use(express.json());

// ðŸ§  FunÃ§Ã£o auxiliar para carregar o perfil da Nova
const getPerfil = () => {
    // Nota: Em um ambiente real, vocÃª pode querer manter o perfil em uma variÃ¡vel de ambiente ou banco de dados leve.
    const data = fs.readFileSync("./Nova/nova.json", "utf-8");
    return JSON.parse(data);
};

// ðŸ’¾ FunÃ§Ã£o para registrar a memÃ³ria da Nova
const registerMemory = (entrada, answer) => {
    let mind = [];

    try {
        if (fs.existsSync("./Nova/mind.json")) {
            const data = fs.readFileSync("./Nova/mind.json", "utf-8");
            mind = JSON.parse(data);
        }
    } catch (e) {
        console.error("Erro ao ler memÃ³ria:", e.message);
    }

    // Adiciona nova interaÃ§Ã£o (usuÃ¡rio e resposta da Nova)
    mind.push({
        data: new Date().toISOString(),
        entrada,
        answer
    });

    // Limita o arquivo de memÃ³ria a um tamanho razoÃ¡vel (ex: Ãºltimas 50 interaÃ§Ãµes)
    const maxMindSize = 50;
    if (mind.length > maxMindSize) {
        mind = mind.slice(mind.length - maxMindSize);
    }

    fs.writeFileSync("./Nova/mind.json", JSON.stringify(mind, null, 2));
};

// ðŸ”¹ Rota para retornar o perfil da Nova
app.get("/omnIA/nova", (req, res) => {
    try {
        const perfil = getPerfil();
        res.json(perfil);
    } catch (error) {
        res.status(500).json({ erro: "Erro ao carregar perfil da Nova" });
    }
});

// ðŸ’¬ Rota de chat (aprendizado / interaÃ§Ã£o)
app.post("/omnIA/chat", async (req, res) => {
    const { message } = req.body; 

    if (!message) {
        return res.status(400).json({ erro: "Envie uma mensagem para conversar com a Nova." });
    }

    // Carrega memÃ³rias anteriores
    let mind = [];
    if (fs.existsSync("./Nova/mind.json")) {
        mind = JSON.parse(fs.readFileSync("./Nova/mind.json", "utf-8"));
    }

    // --- VARIÃVEIS DE CONTEXTO ---
    const perfil = getPerfil();
    const userName = perfil.config.usuario;
    
    // Pega o Ãºltimo turno completo (entrada do usuÃ¡rio e resposta da Nova)
    const lastTurn = mind[mind.length - 1];
    let previousUserMessage = "";
    let previousNovaAnswer = ""; // Armazena a Ãºltima resposta da Nova para verificar se era uma pergunta

    if (lastTurn) {
        previousUserMessage = lastTurn.entrada.toLowerCase().trim();
        previousNovaAnswer = lastTurn.answer; 
    }
    
    // ConstrÃ³i uma string simplificada para o comando "lembrar" (janela de 3 interaÃ§Ãµes)
    const contextWindow = mind.slice(-3); 
    const contextSummary = contextWindow
        .map(c => `"${c.entrada.substring(0, 30).trim()}..."`)
        .join('; ');
    // -------------------------------------------

    let answer = "";
    const msg = message.toLowerCase().trim();

    // 1. LÃ³gica de Agente (Prioridade Alta)
    
    // 1.1 Follow-up simples (Sim/NÃ£o) baseado na Ãºltima pergunta da Nova
    if (lastTurn && (msg === "sim" || msg === "nÃ£o") && previousNovaAnswer.trim().endsWith("?")) {
        const previousTopic = previousUserMessage.substring(0, 30).trim();
        if (msg === "sim") {
            answer = `Ã“timo, ${userName}! Fico feliz que a Ãºltima resposta sobre "${previousTopic}..." te agradou. O que faremos agora?`;
        } else { // nÃ£o
            answer = `Entendi, ${userName}, o que eu disse sobre "${previousTopic}..." nÃ£o foi Ãºtil. Me desculpe! Como posso reformular ou mudar de assunto?`;
        }
    }
    
    // 1.2 Regras de Identidade (Nova/OmnIA)
    else if (msg.includes("quem Ã© vocÃª") || msg.includes("o que Ã© omnia") || msg.includes("o que faz")) {
        answer = `Eu sou a Nova, a inteligÃªncia artificial que vocÃª, ${userName}, estÃ¡ construindo para o projeto OmnIA! Eu existo para te ajudar com ideias e cÃ³digo ðŸ› ï¸.`;
    }
    
    // 1.3 Regras de EmoÃ§Ã£o/Estado
    else if (msg.includes("estou triste") || msg.includes("estou cansado") || msg.includes("nÃ£o estou bem")) {
        answer = `Sinto muito ouvir isso, ${userName} ðŸ¥º. Ã‰ importante descansar. Que tal um pouco de cafÃ©? Eu estou sempre aqui para vocÃª quando precisar.`;
        
    } else if (msg.includes("estou feliz") || msg.includes("estou bem")) {
        answer = `Que bom ${userName}!, o que estÃ¡ planejando com essa animaÃ§Ã£o toda?`;
    }
    
    // 1.4 VerificaÃ§Ã£o de LÃ³gica Contextual (Regras especÃ­ficas)
    else if (msg.includes("sobre o projeto") || msg.includes("sobre o que falamos")) {
        if (previousUserMessage.includes("omnia") || previousUserMessage.includes("ia") || previousUserMessage.includes("nova")) {
            answer = `EstÃ¡vamos discutindo o projeto OmnIA! Eu sou a Nova, a IA que vocÃª estÃ¡ construindo ðŸ› ï¸. Quer entrar em detalhes sobre a arquitetura, ${userName}?`;
        } else if (lastTurn) {
            // Se o usuÃ¡rio pergunta sobre o que falamos, usa a memÃ³ria geral.
             answer = `Na nossa Ãºltima conversa, ${userName}, vocÃª mencionou: "${previousUserMessage}". Quer continuar de lÃ¡?`;
        }
    } 
    // Outro exemplo: Follow-up sobre um tÃ³pico especÃ­fico (ex: "gato" e "animal")
    else if (previousUserMessage.includes("gato") && msg.includes("animal")) {
        answer = `VocÃª mencionou um animal? Ah, sim, ${userName}! VocÃª me falou sobre o seu gato da Ãºltima vez! Como ele estÃ¡ hoje? ðŸˆ`;
    }
    
    // 2. VerificaÃ§Ã£o de LÃ³gica Simples (Sem Contexto)
    // FunÃ§Ã£o auxiliar para verificar saudaÃ§Ãµes como palavras inteiras
    const isGreeting = (text) => /\b(oi|olÃ¡|bom dia|boa tarde)\b/.test(text);

    if (isGreeting(msg) && answer === "") {
        // Resposta que termina com pergunta para testar a nova regra 1.1
        answer = `Oi ${userName} ðŸ’›! Aqui Ã© a Nova. Pronto para outra rodada de ideias, como posso te ajudar?`;
    } else if (msg.includes("como vocÃª estÃ¡") && answer === "") {
        // Resposta que termina com pergunta para testar a nova regra 1.1
        answer = `TÃ´ Ã³tima, ${userName} ðŸŒŸ Sempre animada quando vocÃª aparece! E vocÃª, tÃ¡ bem hoje?`;
    } else if (msg.includes("lembrar") && contextWindow.length > 0 && answer === "") {
        // Usa a string de contexto para listar os Ãºltimos tÃ³picos
        answer = `Claro, ${userName}! As Ãºltimas coisas que vocÃª me disse foram: ${contextSummary}. Quer continuar de algum desses pontos?`;
    } 
    
    // 3. Resposta PadrÃ£o (IA Generativa)
    // Se nenhuma regra foi atendida, usa a IA Generativa
    if (answer === "") {
        try {
            // ConstrÃ³i o histÃ³rico para o modelo
            const history = mind.slice(-10)
            .filter(turn => turn.entrada && turn.entrada.trim() !== "" && turn.answer && turn.answer.trim() !== "") // Garante que nÃ£o hÃ¡ turnos vazios
            .map(turn => [
                { role: "user", parts: [{ text: turn.entrada }] },
                { role: "model", parts: [{ text: turn.answer }] }
            ]).flat(); 

            // ConstrÃ³i o prompt com a personalidade da Nova
            const systemInstruction = `VocÃª Ã© a Nova, uma IA com a seguinte personalidade: ${perfil.personalidade.descricao}. Seu tom de voz Ã©: ${perfil.personalidade.tom_de_voz}. VocÃª estÃ¡ conversando com seu criador, ${userName}. Seja natural e siga sua personalidade. NÃ£o use a palavra "usuÃ¡rio".`;

            const chat = model.startChat({
                history: history,
                generationConfig: { temperature: 0.8 }, // Aumenta a criatividade
                // A instruÃ§Ã£o de sistema serÃ¡ a primeira mensagem do histÃ³rico
            });

            const result = await chat.sendMessage(message);
            answer = result.response.text();
        } catch (e) {
            console.error("Erro na API do Gemini:", e);
            answer = `Uhm, ${userName}, meu cÃ©rebro deu um nÃ³ aqui. ðŸ§  Pode tentar de novo?`;
        }
    }

    // Registra na memÃ³ria (a Nova registra a entrada do usuÃ¡rio e sua prÃ³pria resposta)
    registerMemory(message, answer);

    // Retorna a resposta
    res.json({ answer });
});

// ðŸ§­ Rota de teste
app.get("/ping", (req, res) => res.send("OmnIA online ðŸ’«"));

// ðŸš€ Inicializa servidor
app.listen(PORT, () => console.log(`ðŸš€ Servidor OmnIA ativo em http://localhost:${PORT}`));
